{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290c76a-2990-4f49-b4f1-457ce5ea5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823019d6-354f-497e-8f00-660759779fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28e90e-c3ff-4c2a-b998-a2acd86dbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"openai/gsm8k\"\n",
    "subset='main'\n",
    "trainset=load_dataset(dataset_name,subset,split='train')\n",
    "testset= load_dataset(dataset_name,subset,split='test')\n",
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac558a3-9c3d-4bce-9f34-1eb21a0dd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 10% due to compute availability\n",
    "train_size = int(0.1 * len(trainset))\n",
    "test_size = int(0.1 * len(testset))\n",
    "\n",
    "trainset = trainset.shuffle(seed=42).select(range(train_size))\n",
    "testset = testset.shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "print(f\"Train size: {len(trainset)}\")\n",
    "print(f\"Test size: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c8e58-f63a-4f08-8ff2-ce11cbaf95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a557e-2de0-45cb-a8f2-290b9854d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d91b9-e8c8-467f-8e3b-42db7d28e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_id='Qwen/Qwen3-4B'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              \n",
    "    bnb_4bit_quant_type=\"nf4\",      \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    ")\n",
    "\n",
    "print('model loading with 4 bit quantization')\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"cuda:0\",            \n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435efbe-4efd-4790-a809-9f776f1d17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del trainer\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e16c41-587c-4e09-82f2-080d8827c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\", \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "def format_gsm8k_for_sft(example):\n",
    "    \n",
    "    full_answer = example['answer']\n",
    "    reasoning, solution = full_answer.split(\"####\")\n",
    "    \n",
    "    reasoning = reasoning.strip()\n",
    "    solution = solution.strip()\n",
    "\n",
    "trainset = trainset.map(format_gsm8k_for_sft, remove_columns=trainset.column_names)\n",
    "testset = testset.map(format_gsm8k_for_sft, remove_columns=testset.column_names)\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./primed_baseline\",\n",
    "    max_length=1024,              \n",
    "    dataset_text_field=\"text\",        \n",
    "    per_device_train_batch_size=1,    \n",
    "    gradient_accumulation_steps=8,    \n",
    "    learning_rate=2e-4,               \n",
    "    logging_steps=10,\n",
    "    max_steps=200,                    \n",
    "    save_steps=100,\n",
    "    fp16=False,\n",
    "    bf16=True,                        \n",
    "    report_to=\"none\",                 \n",
    "    packing=False                     \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=trainset,           \n",
    "    eval_dataset=testset,             \n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,          \n",
    ")\n",
    "\n",
    "print(\"Starting SFT\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving Primed Baseline\")\n",
    "# Save the LoRA adapters\n",
    "trainer.model.save_pretrained(\"./primed_baseline_final\")\n",
    "# Save the tokenizer(to be reloaded later)\n",
    "tokenizer.save_pretrained(\"./primed_baseline_final\")\n",
    "print(\"Primed baseline saved to ./primed_baseline_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b18f9-8534-4aef-87ee-86590f656a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text):\n",
    "    answer = text.split(\"</think>\")[-1].strip()\n",
    "    return answer\n",
    "    \n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_answers = [extract_xml_answer(r) for r in responses]\n",
    "    \n",
    "    rewards = []\n",
    "    for predicted, truth in zip(extracted_answers, answer):\n",
    "        pred_clean = re.sub(r\"[^0-9.]\", \"\", predicted)\n",
    "        truth_clean = re.sub(r\"[^0-9.]\", \"\", truth.split(\"####\")[-1])\n",
    "        \n",
    "\n",
    "        if pred_clean == truth_clean and len(pred_clean) > 0:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    rewards = []\n",
    "    for r in responses:\n",
    "        if \"</think>\" in r:\n",
    "            rewards.append(0.2)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def make_prompt(example):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mathematics assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": example['question']},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "grpo_dataset = trainset.map(make_prompt)\n",
    "\n",
    "#Taking loaded in model and adding SFT weights\n",
    "if isinstance(model, PeftModel):\n",
    "    model = model.unload()\n",
    "\n",
    "\n",
    "adapter_path = \"./primed_baseline_final\"\n",
    "print(\"Attaching SFT adapters\")\n",
    "model = PeftModel.from_pretrained(model, adapter_path, is_trainable=True)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"./grpo_gsm8k_final\",\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, \n",
    "    num_generations=2,              \n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=128,      \n",
    "    max_steps=200,                   \n",
    "    save_steps=100,\n",
    "    report_to=\"none\",\n",
    "    use_vllm=False,\n",
    "    gradient_checkpointing=True,    \n",
    "    optim=\"paged_adamw_8bit\"        \n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[correctness_reward_func, format_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=grpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting GRPO Training\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving GRPO Model & Tokenizer\")\n",
    "trainer.model.save_pretrained(\"./grpo_gsm8k_final\")\n",
    "tokenizer.save_pretrained(\"./grpo_gsm8k_final\")\n",
    "print(\"Model and tokenizer saved to ./grpo_gsm8k_final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
